{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DB_HOST=mongodb://localhost/openpath_prod_smart_commute_ebike\n",
      "Config file not found, returning a copy of the environment variables instead...\n",
      "Retrieved config: {'DB_HOST': 'mongodb://localhost/openpath_prod_smart_commute_ebike', 'DB_RESULT_LIMIT': None}\n",
      "Connecting to database URL mongodb://localhost/openpath_prod_smart_commute_ebike\n"
     ]
    }
   ],
   "source": [
    "%env DB_HOST=mongodb://localhost/openpath_prod_smart_commute_ebike\n",
    "import emission.core.get_database as edb\n",
    "import emission.storage.timeseries.aggregate_timeseries as esta\n",
    "import emission.storage.timeseries.builtin_timeseries as estb\n",
    "import emission.core.get_database as gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m pipeline_docs_cursor \u001b[38;5;241m=\u001b[39m gdb\u001b[38;5;241m.\u001b[39mget_timeseries_db()\u001b[38;5;241m.\u001b[39mfind({\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata.key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats/pipeline_time\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m })\n\u001b[0;32m----> 6\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpipeline_docs_cursor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(documents)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display a sample of the documents\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/cursor.py:1248\u001b[0m, in \u001b[0;36mCursor.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__empty:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m-> 1248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_refresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/cursor.py:1188\u001b[0m, in \u001b[0;36mCursor._refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;66;03m# Exhaust cursors don't send getMore messages.\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getmore_class(\n\u001b[1;32m   1175\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__dbname,\n\u001b[1;32m   1176\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__collname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__comment,\n\u001b[1;32m   1187\u001b[0m     )\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data)\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/cursor.py:1052\u001b[0m, in \u001b[0;36mCursor.__send_message\u001b[0;34m(self, operation)\u001b[0m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidOperation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexhaust cursors do not support auto encryption\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1052\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unpack_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__address\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m OperationFailure \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01min\u001b[39;00m _CURSOR_CLOSED_ERRORS \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__exhaust:\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;66;03m# Don't send killCursors because the cursor is already closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/_csot.py:105\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/mongo_client.py:1330\u001b[0m, in \u001b[0;36mMongoClient._run_operation\u001b[0;34m(self, operation, unpack_res, address)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     operation\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Reset op in case of retry.\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m server\u001b[38;5;241m.\u001b[39mrun_operation(\n\u001b[1;32m   1327\u001b[0m         sock_info, operation, read_preference, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_listeners, unpack_res\n\u001b[1;32m   1328\u001b[0m     )\n\u001b[0;32m-> 1330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_cmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Query\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/_csot.py:105\u001b[0m, in \u001b[0;36mapply.<locals>.csot_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _TimeoutContext(timeout):\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/mongo_client.py:1448\u001b[0m, in \u001b[0;36mMongoClient._retryable_read\u001b[0;34m(self, func, read_pref, session, address, retryable)\u001b[0m\n\u001b[1;32m   1446\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m last_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m last_error\n\u001b[0;32m-> 1448\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_pref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retrying:\n\u001b[1;32m   1451\u001b[0m         \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m         \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m         \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/mongo_client.py:1326\u001b[0m, in \u001b[0;36mMongoClient._run_operation.<locals>._cmd\u001b[0;34m(session, server, sock_info, read_preference)\u001b[0m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cmd\u001b[39m(session, server, sock_info, read_preference):\n\u001b[1;32m   1325\u001b[0m     operation\u001b[38;5;241m.\u001b[39mreset()  \u001b[38;5;66;03m# Reset op in case of retry.\u001b[39;00m\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_operation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_listeners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_res\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/server.py:124\u001b[0m, in \u001b[0;36mServer.run_operation\u001b[0;34m(self, sock_info, operation, read_preference, listeners, unpack_res)\u001b[0m\n\u001b[1;32m    122\u001b[0m     user_fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     legacy_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43munpack_res\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegacy_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cmd:\n\u001b[1;32m    132\u001b[0m     first \u001b[38;5;241m=\u001b[39m docs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/cursor.py:1120\u001b[0m, in \u001b[0;36mCursor._unpack_response\u001b[0;34m(self, response, cursor_id, codec_options, user_fields, legacy_response)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unpack_response\u001b[39m(\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28mself\u001b[39m, response, cursor_id, codec_options, user_fields\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, legacy_response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m ):\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcursor_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegacy_response\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/pymongo/message.py:1407\u001b[0m, in \u001b[0;36m_OpMsg.unpack_response\u001b[0;34m(self, cursor_id, codec_options, user_fields, legacy_response)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;66;03m# If _OpMsg is in-use, this cannot be a legacy response.\u001b[39;00m\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m legacy_response\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode_all_selective\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpayload_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/bson/__init__.py:1208\u001b[0m, in \u001b[0;36m_decode_all_selective\u001b[0;34m(data, codec_options, fields)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode BSON data to a single document while using user-provided\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;124;03mcustom decoding logic.\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;124;03m.. versionadded:: 3.8\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m codec_options\u001b[38;5;241m.\u001b[39mtype_registry\u001b[38;5;241m.\u001b[39m_decoder_map:\n\u001b[0;32m-> 1208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecode_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fields:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decode_all(data, codec_options\u001b[38;5;241m.\u001b[39mwith_options(type_registry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/bson/__init__.py:1122\u001b[0m, in \u001b[0;36mdecode_all\u001b[0;34m(data, codec_options)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opts, CodecOptions):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _CODEC_OPTIONS_TYPE_ERROR\n\u001b[0;32m-> 1122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_decode_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/emission_copy/lib/python3.9/site-packages/bson/objectid.py:60\u001b[0m, in \u001b[0;36mObjectId.__init__\u001b[0;34m(self, oid)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;18m__slots__\u001b[39m \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__id\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n\u001b[1;32m     58\u001b[0m _type_marker \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, oid: Optional[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObjectId\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mbytes\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a new ObjectId.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    An ObjectId is a 12-byte unique identifier consisting of:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03m       objectid.rst>`_.\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m oid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline_docs_cursor = gdb.get_timeseries_db().find({\n",
    "    \"metadata.key\": \"stats/pipeline_time\",\n",
    "})\n",
    "\n",
    "\n",
    "documents = list(pipeline_docs_cursor)\n",
    "count = len(documents)\n",
    "print(f'count = {count}')\n",
    "# Display a sample of the documents\n",
    "import pprint\n",
    "pipeline_docs = list(pipeline_docs_cursor)\n",
    "if pipeline_docs:\n",
    "    single_doc = pipeline_docs[0]\n",
    "    print(\"Single Document:\")\n",
    "    pprint.pprint(single_doc)\n",
    "else:\n",
    "    print(\"No documents found for 'stats/pipeline_time'.\")\n",
    "\n",
    "# Fetch multiple documents\n",
    "pipeline_docs_sample = pipeline_docs[:5]  # Get first 5 documents\n",
    "print(\"\\nMultiple Documents:\")\n",
    "for doc in pipeline_docs_sample:\n",
    "    pprint.pprint(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "df = pd.json_normalize(pipeline_docs)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = df['data.name'].unique()\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Step 1: Filter for rows where data.name is \"USERCACHE\"\n",
    "# usercache_df = df[df['data.name'] == \"USERCACHE\"]\n",
    "\n",
    "# # Step 2: Convert metadata.write_ts to datetime\n",
    "# usercache_df['datetime'] = pd.to_datetime(usercache_df['metadata.write_ts'], unit='s')\n",
    "\n",
    "# # Step 3: Define the start date for filtering\n",
    "# start_date = pd.Timestamp('2024-11-08')  # Adjust as needed\n",
    "\n",
    "# # Step 4: Filter for rows since the start date\n",
    "# usercache_df = usercache_df[usercache_df['datetime'] >= start_date]\n",
    "\n",
    "# # Step 5: Group by hour and count executions\n",
    "# hourly_execution_counts = usercache_df.groupby(usercache_df['datetime'].dt.floor('H')).size()\n",
    "\n",
    "# # Step 6: Output the results\n",
    "# if hourly_execution_counts.empty:\n",
    "#     print(\"No executions of 'USERCACHE' since November 8.\")\n",
    "# else:\n",
    "#     print(\"Hourly execution counts since November 8:\")\n",
    "#     print(hourly_execution_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# Step 0: Define the list of 'data.name' entries to exclude\n",
    "# These are the 'Parent' functions\n",
    "exclude_data_names = [\n",
    "    'TRIP_SEGMENTATION/segment_into_trips',\n",
    "    'TRIP_SEGMENTATION/segment_into_trips_dist/loop'\n",
    "]\n",
    "\n",
    "# Step 1: Filter for function-level data only (entries with slashes in 'data.name') and exclude specified names\n",
    "function_level_df = df[\n",
    "    df['data.name'].str.contains('/') &\n",
    "    ~df['data.name'].isin(exclude_data_names)\n",
    "].copy()\n",
    "\n",
    "# Step 2: Select the relevant columns\n",
    "selected_columns = function_level_df[['data.reading', 'data.name']].copy()\n",
    "\n",
    "# Step 3: Data Cleaning\n",
    "# Drop rows with missing values in 'data.reading' or 'data.name'\n",
    "selected_columns.dropna(subset=['data.reading', 'data.name'], inplace=True)\n",
    "\n",
    "# Ensure 'data.reading' is numeric\n",
    "selected_columns = selected_columns[pd.to_numeric(selected_columns['data.reading'], errors='coerce').notnull()]\n",
    "\n",
    "\n",
    "# Step 5: Aggregate 'data.reading' by 'data.name'\n",
    "\n",
    "# Aggregation Using Sum\n",
    "aggregated_sum = selected_columns.groupby('data.name', as_index=False)['data.reading'].sum()\n",
    "aggregated_sum.rename(columns={'data.reading': 'total_reading'}, inplace=True)\n",
    "\n",
    "# Aggregation Using Mean\n",
    "aggregated_mean = selected_columns.groupby('data.name', as_index=False)['data.reading'].mean()\n",
    "aggregated_mean.rename(columns={'data.reading': 'average_reading'}, inplace=True)\n",
    "\n",
    "# Step 6: Determine the 80th percentile threshold based on aggregated values\n",
    "\n",
    "# For Sum Aggregation\n",
    "threshold_sum = aggregated_sum['total_reading'].quantile(0.80)\n",
    "\n",
    "# For Mean Aggregation\n",
    "threshold_mean = aggregated_mean['average_reading'].quantile(0.80)\n",
    "\n",
    "# For Total Aggregation\n",
    "threshold_total = selected_columns['data.reading'].quantile(0.80)\n",
    "\n",
    "# Step 7: Split the DataFrame into top 20% and bottom 80% based on aggregated values\n",
    "\n",
    "# Using Sum Aggregation\n",
    "top20_sum = aggregated_sum[aggregated_sum['total_reading'] >= threshold_sum].sort_values(by='total_reading', ascending=False)\n",
    "bottom80_sum = aggregated_sum[aggregated_sum['total_reading'] < threshold_sum].sort_values(by='total_reading', ascending=False)\n",
    "top20_total = selected_columns[selected_columns['data.reading'] >= threshold_total].sort_values(by='data.reading', ascending=False)\n",
    "bottom80_total = selected_columns[selected_columns['data.reading'] < threshold_total].sort_values(by='data.reading', ascending=False)\n",
    "\n",
    "# Using Mean Aggregation\n",
    "top20_mean = aggregated_mean[aggregated_mean['average_reading'] >= threshold_mean].sort_values(by='average_reading', ascending=False)\n",
    "bottom80_mean = aggregated_mean[aggregated_mean['average_reading'] < threshold_mean].sort_values(by='average_reading', ascending=False)\n",
    "\n",
    "# Step 8: Define the base directory and file paths\n",
    "base_dir = os.getcwd()  # Current working directory\n",
    "\n",
    "# Paths for Sum Aggregation\n",
    "aggregated_sum_path = os.path.join(base_dir, 'aggregated_sum_function_level_2.csv')\n",
    "top20_sum_path = os.path.join(base_dir, 'top20_function_level_sum_sorted_2.csv')\n",
    "bottom80_sum_path = os.path.join(base_dir, 'bottom80_function_level_sum_sorted_2.csv')\n",
    "top20_total_path = os.path.join(base_dir, 'top20_function_level_sum_sorted_2.csv')\n",
    "bottom80_total_path = os.path.join(base_dir, 'bottm80_function_level_sum_sorted_2.csv')\n",
    "\n",
    "# Paths for Mean Aggregation\n",
    "aggregated_mean_path = os.path.join(base_dir, 'aggregated_mean_function_level_2.csv')\n",
    "top20_mean_path = os.path.join(base_dir, 'top20_function_level_mean_sorted_2.csv')\n",
    "bottom80_mean_path = os.path.join(base_dir, 'bottom80_function_level_mean_sorted_2.csv')\n",
    "\n",
    "# Step 9: Save the aggregated and categorized DataFrames to CSV files\n",
    "\n",
    "# Saving Sum Aggregation\n",
    "aggregated_sum.to_csv(aggregated_sum_path, index=False)\n",
    "top20_sum.to_csv(top20_sum_path, index=False)\n",
    "bottom80_sum.to_csv(bottom80_sum_path, index=False)\n",
    "top20_total.to_csv(top20_total_path, index=False)\n",
    "bottom80_total.to_csv(bottom80_total_path, index=False)\n",
    "\n",
    "print(f\"Aggregated Sum Function-Level Data saved to {aggregated_sum_path}\")\n",
    "print(f\"Top 20% (Sum) function-level data saved to {top20_sum_path}\")\n",
    "print(f\"Bottom 80% (Sum) function-level data saved to {bottom80_sum_path}\")\n",
    "print(f\"Top 20%  function-level data saved to {top20_total_path}\")\n",
    "print(f\"Bottom 80% function-level data saved to {bottom80_total_path}\")\n",
    "\n",
    "# Saving Mean Aggregation\n",
    "aggregated_mean.to_csv(aggregated_mean_path, index=False)\n",
    "top20_mean.to_csv(top20_mean_path, index=False)\n",
    "bottom80_mean.to_csv(bottom80_mean_path, index=False)\n",
    "\n",
    "print(f\"\\nAggregated Mean Function-Level Data saved to {aggregated_mean_path}\")\n",
    "print(f\"Top 20% (Mean) function-level data saved to {top20_mean_path}\")\n",
    "print(f\"Bottom 80% (Mean) function-level data saved to {bottom80_mean_path}\")\n",
    "\n",
    "# Step 10: Verify the splits\n",
    "print(f\"\\nSum Aggregation - Top 20% row count: {len(top20_sum)}\")\n",
    "print(f\"Sum Aggregation - Bottom 80% row count: {len(bottom80_sum)}\")\n",
    "\n",
    "print(f\"\\nMean Aggregation - Top 20% row count: {len(top20_mean)}\")\n",
    "print(f\"Mean Aggregation - Bottom 80% row count: {len(bottom80_mean)}\")\n",
    "\n",
    "# Step 11: Inspect some entries\n",
    "print(\"\\nSample Top 20% Sum Aggregation Entries:\")\n",
    "print(top20_sum.head())\n",
    "\n",
    "print(\"\\nSample Bottom 80% Sum Aggregation Entries:\")\n",
    "print(bottom80_sum.head())\n",
    "\n",
    "print(\"\\nSample Top 20% Mean Aggregation Entries:\")\n",
    "print(top20_mean.head())\n",
    "\n",
    "print(\"\\nSample Bottom 80% Mean Aggregation Entries:\")\n",
    "print(bottom80_mean.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pipeline_time(combined_df):\n",
    "    avg_time_df = combined_df.groupby('data.name')['data.reading'].mean().reset_index()\n",
    "    avg_time_df.rename(columns={'data.reading': 'average_time'}, inplace=True)\n",
    "    \n",
    "    print(\"\\nAverage Pipeline Time per Step:\")\n",
    "    print(avg_time_df.sort_values(by='average_time', ascending=False))\n",
    "    \n",
    "    # Optionally, save to CSV\n",
    "    avg_time_df.to_csv('average_pipeline_time_per_step.csv', index=False)\n",
    "\n",
    "average_pipeline_time(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def pipeline_time_distribution(combined_df, step_name):\n",
    "    step_df = combined_df[combined_df['data.name'] == step_name]\n",
    "    \n",
    "    if step_df.empty:\n",
    "        print(f\"No data found for step: {step_name}\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(step_df['data.reading'], bins=30, kde=True)\n",
    "    plt.title(f\"Distribution of Pipeline Times for {step_name}\")\n",
    "    plt.xlabel(\"Pipeline Time (seconds)\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "pipeline_time_distribution(df, 'TRIP_SEGMENTATION/segment_into_trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_time_trends(combined_df, step_name, freq='D'):\n",
    "    \"\"\"\n",
    "    Plots the trend of pipeline times over time for a specific step.\n",
    "    :param freq: Resampling frequency ('D' for daily, 'W' for weekly, 'M' for monthly)\n",
    "    \"\"\"\n",
    "    step_df = combined_df[combined_df['data.name'] == step_name].copy()\n",
    "    \n",
    "    if step_df.empty:\n",
    "        print(f\"No data found for step: {step_name}\")\n",
    "        return\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    step_df['datetime'] = pd.to_datetime(step_df['metadata.write_ts'], unit='s')\n",
    "    \n",
    "    # Set datetime as index\n",
    "    step_df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    # Resample and calculate mean pipeline time\n",
    "    resampled = step_df['data.reading'].resample(freq).mean()\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12,6))\n",
    "    resampled.plot()\n",
    "    plt.title(f\"Trend of Pipeline Times Over Time for {step_name}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Average Pipeline Time (seconds)\")\n",
    "    plt.show()\n",
    "\n",
    "pipeline_time_trends(df, 'TRIP_SEGMENTATION', 'W')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bottlenecks(combined_df, top_n=5):\n",
    "    avg_time_df = combined_df.groupby('data.name')['data.reading'].mean().reset_index()\n",
    "    avg_time_df.rename(columns={'data.reading': 'average_time'}, inplace=True)\n",
    "    \n",
    "    bottlenecks = avg_time_df.sort_values(by='average_time', ascending=False).head(top_n)\n",
    "    \n",
    "    print(f\"\\nTop {top_n} Bottleneck Pipeline Steps:\")\n",
    "    print(bottlenecks)\n",
    "    \n",
    "    # Optionally, visualize\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.barplot(x='average_time', y='data.name', data=bottlenecks, palette='viridis')\n",
    "    plt.title(f\"Top {top_n} Bottleneck Pipeline Steps by Average Time\")\n",
    "    plt.xlabel(\"Average Pipeline Time (seconds)\")\n",
    "    plt.ylabel(\"Pipeline Step\")\n",
    "    plt.show()\n",
    "\n",
    "identify_bottlenecks(function_level_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_pipeline_times(combined_df, step_name):\n",
    "    step_df = combined_df[combined_df['data.name'] == step_name].copy()\n",
    "    \n",
    "    if step_df.empty:\n",
    "        print(f\"No data found for step: {step_name}\")\n",
    "        return\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    step_df['datetime'] = pd.to_datetime(step_df['metadata.write_ts'], unit='s')\n",
    "    \n",
    "    # Extract hour and day of week\n",
    "    step_df['hour'] = step_df['datetime'].dt.hour\n",
    "    step_df['day_of_week'] = step_df['datetime'].dt.day_name()\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot = step_df.pivot_table(values='data.reading', index='day_of_week', columns='hour', aggfunc='mean')\n",
    "    \n",
    "    # Reorder days of the week\n",
    "    days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    pivot = pivot.reindex(days_order)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(15,7))\n",
    "    sns.heatmap(pivot, cmap='YlGnBu', annot=True, fmt=\".2f\")\n",
    "    plt.title(f\"Heatmap of Average Pipeline Times for {step_name}\")\n",
    "    plt.xlabel(\"Hour of Day\")\n",
    "    plt.ylabel(\"Day of Week\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "heatmap_pipeline_times(df, 'TRIP_SEGMENTATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execution_time_variability_per_step(df):\n",
    "    variability_df = df.groupby('data.name')['data.reading'].agg(['mean', 'std', 'var']).reset_index()\n",
    "    variability_df.rename(columns={'mean': 'average_time_sec', 'std': 'std_dev_sec', 'var': 'variance_sec2'}, inplace=True)\n",
    "    \n",
    "    # Sort by standard deviation descending\n",
    "    variability_df = variability_df.sort_values(by='std_dev_sec', ascending=False)\n",
    "    \n",
    "    print(\"\\nExecution Time Variability per Pipeline Step:\")\n",
    "    print(variability_df)\n",
    "    \n",
    "    # Visualization: Box Plots to visualize variability\n",
    "    plt.figure(figsize=(14,10))\n",
    "    sns.boxplot(x='data.reading', y='data.name', data=df, palette='coolwarm')\n",
    "    plt.title(\"Execution Time Variability per Pipeline Step\")\n",
    "    plt.xlabel(\"Execution Time (seconds)\")\n",
    "    plt.ylabel(\"Pipeline Step\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save to CSV\n",
    "    variability_df.to_csv('execution_time_variability_per_step.csv', index=False)\n",
    "    print(\"Saved execution time variability to 'execution_time_variability_per_step.csv'\")\n",
    "\n",
    "\n",
    "execution_time_variability_per_step(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_contribution_analysis(df):\n",
    "    # Calculate total pipeline time per execution\n",
    "    total_time_per_execution = df.groupby(['user_id', 'metadata.key'])['data.reading'].sum().reset_index()\n",
    "    total_time_per_execution.rename(columns={'data.reading': 'total_pipeline_time_sec'}, inplace=True)\n",
    "    \n",
    "    # Merge total pipeline time back to the main DataFrame\n",
    "    df = df.merge(total_time_per_execution, on=['user_id', 'metadata.key'])\n",
    "    \n",
    "    # Calculate proportion of each step's time to the total pipeline time\n",
    "    df['step_proportion_percent'] = (df['data.reading'] / df['total_pipeline_time_sec']) * 100\n",
    "    \n",
    "    # Aggregate average proportion per step\n",
    "    step_contribution_df = df.groupby('data.name')['step_proportion_percent'].mean().reset_index()\n",
    "    step_contribution_df.rename(columns={'step_proportion_percent': 'average_proportion_percent'}, inplace=True)\n",
    "    \n",
    "    # Sort descending\n",
    "    step_contribution_df = step_contribution_df.sort_values(by='average_proportion_percent', ascending=False)\n",
    "    \n",
    "    print(\"\\nAverage Proportion of Total Pipeline Time per Step:\")\n",
    "    print(step_contribution_df)\n",
    "    \n",
    "    # Visualization: Horizontal Bar Chart\n",
    "    plt.figure(figsize=(12,8))\n",
    "    sns.barplot(x='average_proportion_percent', y='data.name', data=step_contribution_df, palette='coolwarm')\n",
    "    plt.title(\"Average Proportion of Total Pipeline Time per Step\")\n",
    "    plt.xlabel(\"Average Proportion (%)\")\n",
    "    plt.ylabel(\"Pipeline Step\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save to CSV\n",
    "    step_contribution_df.to_csv('average_proportion_total_time_per_step.csv', index=False)\n",
    "    print(\"Saved step contribution analysis to 'average_proportion_total_time_per_step.csv'\")\n",
    "\n",
    "step_contribution_analysis(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
