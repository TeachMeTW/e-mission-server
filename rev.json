{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Processing `stats/pipeline_time` Data for All Users\n",
       "\n",
       "This notebook retrieves and analyzes the `stats/pipeline_time` data for all users in the `e-mission` database. The analysis includes fetching data, aggregating it, and performing specific analyses such as `USERCACHE` executions and function-level data processing."
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Setup Environment\n",
       "\n",
       "Ensure that the `e-mission` environment is correctly set up. Use the provided bash script to launch a Jupyter notebook with the correct `PYTHONPATH`:\n",
       "\n",
       "```bash\n",
       "$ ./e-mission-jupyter.bash notebook\n",
       "```"
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Import Necessary Libraries and Modules"
     },
     {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Import necessary libraries and modules\n",
       "import emission.core.get_database as edb\n",
       "import emission.storage.timeseries.abstract_timeseries as esta\n",
       "import emission.storage.decorations.analysis_timeseries_queries as esda\n",
       "import emission.core.wrapper.entry as ecwe\n",
       "import emission.storage.decorations.trip_queries as esdt\n",
       "import emission.storage.timeseries.timequery as estt\n",
       "import pandas as pd\n",
       "from datetime import datetime, timedelta\n",
       "import pytz\n",
       "import pprint\n",
       "import os\n",
       "import logging\n"
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Configure Logging\n",
       "\n",
       "Set up logging to capture informational, warning, and error messages."
     },
     {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Configure logging\n",
       "logging.basicConfig(level=logging.INFO)\n",
       "logger = logging.getLogger(__name__)"
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Retrieve All User UUIDs\n",
       "\n",
       "Define a function to fetch all user UUIDs from the database."
     },
     {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
       "def get_all_user_uuids():\n",
       "    \"\"\"\n",
       "    Retrieves a list of all user UUIDs from the database.\n",
       "    \"\"\"\n",
       "    try:\n",
       "        uuid_cursor = edb.get_uuid_db().find({}, {\"uuid\": 1, \"_id\": 0})\n",
       "        uuid_list = [doc['uuid'] for doc in uuid_cursor]\n",
       "        logger.info(f\"Retrieved {len(uuid_list)} user UUIDs.\")\n",
       "        return uuid_list\n",
       "    except Exception as e:\n",
       "        logger.error(f\"Error retrieving user UUIDs: {e}\")\n",
       "        return []"
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Fetch `stats/pipeline_time` Data for All Users\n",
       "\n",
       "Define a function to iterate over each user UUID, fetch their `stats/pipeline_time` data, and compile it into a list of DataFrames."
     },
     {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
       "def fetch_pipeline_time_data(uuid_list):\n",
       "    \"\"\"\n",
       "    Fetches 'stats/pipeline_time' data for all users.\n",
       "    \"\"\"\n",
       "    all_users_pipeline_dfs = []\n",
       "    for idx, user_uuid in enumerate(uuid_list, start=1):\n",
       "        try:\n",
       "            ts = esta.TimeSeries.get_time_series(user_uuid)\n",
       "            pipeline_df = ts.get_data_df(\"stats/pipeline_time\", time_query=None)\n",
       "            if not pipeline_df.empty:\n",
       "                pipeline_df['user_uuid'] = user_uuid  # Add user identifier\n",
       "                all_users_pipeline_dfs.append(pipeline_df)\n",
       "                logger.info(f\"[{idx}/{len(uuid_list)}] Fetched data for user {user_uuid}.\")\n",
       "            else:\n",
       "                logger.info(f\"[{idx}/{len(uuid_list)}] No 'stats/pipeline_time' data for user {user_uuid}.\")\n",
       "        except Exception as e:\n",
       "            logger.error(f\"[{idx}/{len(uuid_list)}] Error fetching data for user {user_uuid}: {e}\")\n",
       "    return all_users_pipeline_dfs"
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Analyze `USERCACHE` Executions\n",
       "\n",
       "Define a function to analyze `USERCACHE` executions since a specified start date."
     },
     {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
       "def analyze_usercache(combined_df, start_date_str='2024-11-08'):\n",
       "    \"\"\"\n",
       "    Analyzes 'USERCACHE' executions since a specified start date.\n",
       "    \"\"\"\n",
       "    logger.info(\"\\nAnalyzing 'USERCACHE' executions...\")\n",
       "    # Step 1: Filter for rows where data.name is \"USERCACHE\"\n",
       "    usercache_df = combined_df[combined_df['data.name'] == \"USERCACHE\"].copy()\n",
       "\n",
       "    if usercache_df.empty:\n",
       "        logger.info(\"No 'USERCACHE' entries found.\")\n",
       "        return\n",
       "\n",
       "    # Step 2: Convert metadata.write_ts to datetime\n",
       "    usercache_df['datetime'] = pd.to_datetime(usercache_df['metadata.write_ts'], unit='s')\n",
       "\n",
       "    # Step 3: Define the start date for filtering\n",
       "    start_date = pd.Timestamp(start_date_str)\n",
       "\n",
       "    # Step 4: Filter for rows since the start date\n",
       "    usercache_df = usercache_df[usercache_df['datetime'] >= start_date]\n",
       "\n",
       "    # Step 5: Group by hour and count executions\n",
       "    hourly_execution_counts = usercache_df.groupby(usercache_df['datetime'].dt.floor('H')).size()\n",
       "\n",
       "    # Step 6: Output the results\n",
       "    if hourly_execution_counts.empty:\n",
       "        logger.info(f\"No executions of 'USERCACHE' since {start_date_str}.\")\n",
       "    else:\n",
       "        logger.info(f\"Hourly execution counts since {start_date_str}:\")\n",
       "        logger.info(f\"\\n{hourly_execution_counts}\")"
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Process Function-Level Data\n",
       "\n",
       "Define a function to process function-level data by filtering, aggregating, and saving the results to CSV files."
     },
     {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
       "def process_function_level_data(combined_df, exclude_names, base_dir=os.getcwd()):\n",
       "    \"\"\"\n",
       "    Processes function-level data by filtering, aggregating, and saving to CSV files.\n",
       "    \"\"\"\n",
       "    logger.info(\"\\nProcessing function-level data...\")\n",
       "    # Step 1: Filter for function-level data only (entries with slashes in 'data.name') and exclude specified names\n",
       "    function_level_df = combined_df[\n",
       "        combined_df['data.name'].str.contains('/') &\n",
       "        ~combined_df['data.name'].isin(exclude_names)\n",
       "    ].copy()\n",
       "\n",
       "    if function_level_df.empty:\n",
       "        logger.warning(\"No function-level data after filtering.\")\n",
       "        return\n",
       "\n",
       "    # Step 2: Select the relevant columns\n",
       "    selected_columns = function_level_df[['data.reading', 'data.name']].copy()\n",
       "\n",
       "    # Step 3: Data Cleaning\n",
       "    selected_columns.dropna(subset=['data.reading', 'data.name'], inplace=True)\n",
       "    selected_columns = selected_columns[pd.to_numeric(selected_columns['data.reading'], errors='coerce').notnull()]\n",
       "\n",
       "    if selected_columns.empty:\n",
       "        logger.warning(\"No valid 'data.reading' after cleaning.\")\n",
       "        return\n",
       "\n",
       "    # Step 4: Aggregate 'data.reading' by 'data.name'\n",
       "    aggregated_sum = selected_columns.groupby('data.name', as_index=False)['data.reading'].sum()\n",
       "    aggregated_sum.rename(columns={'data.reading': 'total_reading'}, inplace=True)\n",
       "\n",
       "    aggregated_mean = selected_columns.groupby('data.name', as_index=False)['data.reading'].mean()\n",
       "    aggregated_mean.rename(columns={'data.reading': 'average_reading'}, inplace=True)\n",
       "\n",
       "    # Step 5: Determine the 80th percentile threshold\n",
       "    threshold_sum = aggregated_sum['total_reading'].quantile(0.80)\n",
       "    threshold_mean = aggregated_mean['average_reading'].quantile(0.80)\n",
       "\n",
       "    # Step 6: Split the DataFrame into top 20% and bottom 80%\n",
       "    top20_sum = aggregated_sum[aggregated_sum['total_reading'] >= threshold_sum].sort_values(by='total_reading', ascending=False)\n",
       "    bottom80_sum = aggregated_sum[aggregated_sum['total_reading'] < threshold_sum].sort_values(by='total_reading', ascending=False)\n",
       "\n",
       "    top20_mean = aggregated_mean[aggregated_mean['average_reading'] >= threshold_mean].sort_values(by='average_reading', ascending=False)\n",
       "    bottom80_mean = aggregated_mean[aggregated_mean['average_reading'] < threshold_mean].sort_values(by='average_reading', ascending=False)\n",
       "\n",
       "    # Step 7: Define file paths\n",
       "    aggregated_sum_path = os.path.join(base_dir, 'aggregated_sum_function_level.csv')\n",
       "    top20_sum_path = os.path.join(base_dir, 'top20_function_level_sum_sorted.csv')\n",
       "    bottom80_sum_path = os.path.join(base_dir, 'bottom80_function_level_sum_sorted.csv')\n",
       "\n",
       "    aggregated_mean_path = os.path.join(base_dir, 'aggregated_mean_function_level.csv')\n",
       "    top20_mean_path = os.path.join(base_dir, 'top20_function_level_mean_sorted.csv')\n",
       "    bottom80_mean_path = os.path.join(base_dir, 'bottom80_function_level_mean_sorted.csv')\n",
       "\n",
       "    # Step 8: Save to CSV\n",
       "    try:\n",
       "        aggregated_sum.to_csv(aggregated_sum_path, index=False)\n",
       "        top20_sum.to_csv(top20_sum_path, index=False)\n",
       "        bottom80_sum.to_csv(bottom80_sum_path, index=False)\n",
       "\n",
       "        aggregated_mean.to_csv(aggregated_mean_path, index=False)\n",
       "        top20_mean.to_csv(top20_mean_path, index=False)\n",
       "        bottom80_mean.to_csv(bottom80_mean_path, index=False)\n",
       "\n",
       "        logger.info(f\"Aggregated Sum Function-Level Data saved to {aggregated_sum_path}\")\n",
       "        logger.info(f\"Top 20% (Sum) function-level data saved to {top20_sum_path}\")\n",
       "        logger.info(f\"Bottom 80% (Sum) function-level data saved to {bottom80_sum_path}\")\n",
       "\n",
       "        logger.info(f\"\\nAggregated Mean Function-Level Data saved to {aggregated_mean_path}\")\n",
       "        logger.info(f\"Top 20% (Mean) function-level data saved to {top20_mean_path}\")\n",
       "        logger.info(f\"Bottom 80% (Mean) function-level data saved to {bottom80_mean_path}\")\n",
       "    except Exception as e:\n",
       "        logger.error(f\"Error saving aggregated data to CSV: {e}\")\n",
       "        return\n",
       "\n",
       "    # Step 9: Verify the splits\n",
       "    logger.info(f\"\\nSum Aggregation - Top 20% row count: {len(top20_sum)}\")\n",
       "    logger.info(f\"Sum Aggregation - Bottom 80% row count: {len(bottom80_sum)}\")\n",
       "\n",
       "    logger.info(f\"\\nMean Aggregation - Top 20% row count: {len(top20_mean)}\")\n",
       "    logger.info(f\"Mean Aggregation - Bottom 80% row count: {len(bottom80_mean)}\")\n",
       "\n",
       "    # Step 10: Inspect some entries\n",
       "    logger.info(\"\\nSample Top 20% Sum Aggregation Entries:\")\n",
       "    logger.info(f\"\\n{top20_sum.head()}\")\n",
       "\n",
       "    logger.info(\"\\nSample Bottom 80% Sum Aggregation Entries:\")\n",
       "    logger.info(f\"\\n{bottom80_sum.head()}\")\n",
       "\n",
       "    logger.info(\"\\nSample Top 20% Mean Aggregation Entries:\")\n",
       "    logger.info(f\"\\n{top20_mean.head()}\")\n",
       "\n",
       "    logger.info(\"\\nSample Bottom 80% Mean Aggregation Entries:\")\n",
       "    logger.info(f\"\\n{bottom80_mean.head()}\")"
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Aggregate and Analyze Data Across All Users\n",
       "\n",
       "Define a function to combine all users' DataFrames and perform the analysis."
     },
     {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
       "def aggregate_data(all_users_pipeline_dfs, exclude_names, base_dir=os.getcwd()):\n",
       "    \"\"\"\n",
       "    Aggregates the 'stats/pipeline_time' data from all users and performs the required analysis.\n",
       "    \"\"\"\n",
       "    if not all_users_pipeline_dfs:\n",
       "        logger.warning(\"No pipeline data to aggregate.\")\n",
       "        return\n",
       "\n",
       "    # Combine all users' DataFrames\n",
       "    combined_pipeline_df = pd.concat(all_users_pipeline_dfs, ignore_index=True)\n",
       "    logger.info(f\"Combined pipeline data shape: {combined_pipeline_df.shape}\")\n",
       "\n",
       "    # Describe and get info about the combined DataFrame\n",
       "    logger.info(\"Combined Pipeline Data Description:\")\n",
       "    logger.info(f\"\\n{combined_pipeline_df.describe()}\")\n",
       "\n",
       "    logger.info(\"Combined Pipeline Data Info:\")\n",
       "    buffer = []\n",
       "    combined_pipeline_df.info(buf=buffer)\n",
       "    for line in buffer:\n",
       "        logger.info(line)\n",
       "\n",
       "    # Get unique 'data.name' entries\n",
       "    unique_names = combined_pipeline_df['data.name'].unique()\n",
       "    logger.info(f\"Unique 'data.name' entries: {unique_names}\")\n",
       "\n",
       "    # Analyze 'USERCACHE' executions\n",
       "    analyze_usercache(combined_pipeline_df)\n",
       "\n",
       "    # Process function-level data\n",
       "    process_function_level_data(combined_pipeline_df, exclude_names, base_dir)"
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Main Execution Flow\n",
       "\n",
       "Define the main function to orchestrate the data retrieval and analysis."
     },
     {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
       "def main():\n",
       "    # Step 1: Retrieve all user UUIDs\n",
       "    user_uuid_list = get_all_user_uuids()\n",
       "\n",
       "    if not user_uuid_list:\n",
       "        logger.error(\"No user UUIDs retrieved. Exiting script.\")\n",
       "        return\n",
       "\n",
       "    # Step 2: Fetch 'stats/pipeline_time' data for all users\n",
       "    all_users_pipeline_dfs = fetch_pipeline_time_data(user_uuid_list)\n",
       "\n",
       "    if not all_users_pipeline_dfs:\n",
       "        logger.warning(\"No pipeline data fetched for any user.\")\n",
       "        return\n",
       "\n",
       "    # Step 3: Combine all users' DataFrames\n",
       "    combined_pipeline_df = pd.concat(all_users_pipeline_dfs, ignore_index=True)\n",
       "    logger.info(f\"\\nCombined Pipeline Data Shape: {combined_pipeline_df.shape}\")\n",
       "\n",
       "    # Step 4: Describe and get info about the combined DataFrame\n",
       "    logger.info(\"\\nCombined Pipeline Data Description:\")\n",
       "    logger.info(f\"\\n{combined_pipeline_df.describe()}\")\n",
       "\n",
       "    logger.info(\"\\nCombined Pipeline Data Info:\")\n",
       "    buffer = []\n",
       "    combined_pipeline_df.info(buf=buffer)\n",
       "    for line in buffer:\n",
       "        logger.info(line)\n",
       "\n",
       "    # Step 5: Get unique 'data.name' entries\n",
       "    unique_names = combined_pipeline_df['data.name'].unique()\n",
       "    logger.info(f\"\\nUnique 'data.name' entries:\")\n",
       "    logger.info(unique_names)\n",
       "\n",
       "    # Step 6: Analyze 'USERCACHE' executions\n",
       "    analyze_usercache(combined_pipeline_df)\n",
       "\n",
       "    # Step 7: Define the list of 'data.name' entries to exclude\n",
       "    exclude_data_names = [\n",
       "        'TRIP_SEGMENTATION/segment_into_trips',\n",
       "        'TRIP_SEGMENTATION/segment_into_trips_dist/loop'\n",
       "    ]\n",
       "\n",
       "    # Step 8: Process function-level data\n",
       "    process_function_level_data(combined_pipeline_df, exclude_data_names)\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    main()"
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## Execute the Script\n",
       "\n",
       "Run the main function to execute the data retrieval and analysis."
     },
     {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Execute the main function\n",
       "main()"
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 5
   }
   